<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://ellis-life-heidelberg.eu//feed.xml" rel="self" type="application/atom+xml" /><link href="https://ellis-life-heidelberg.eu//" rel="alternate" type="text/html" /><updated>2021-06-15T23:36:08+00:00</updated><id>https://ellis-life-heidelberg.eu//feed.xml</id><title type="html">ELLIS Life Heidelberg</title><subtitle>ELLIS Life Heidelberg Unit Website.</subtitle><entry><title type="html">ELLIS Life / NCT Data Science Seminar: Dmitry Kobak</title><link href="https://ellis-life-heidelberg.eu//2021/06/16/ellis-life-data-science-seminar-dmitry-kobak.html" rel="alternate" type="text/html" title="ELLIS Life / NCT Data Science Seminar: Dmitry Kobak" /><published>2021-06-16T00:00:00+00:00</published><updated>2021-06-16T00:00:00+00:00</updated><id>https://ellis-life-heidelberg.eu//2021/06/16/ellis-life-data-science-seminar-dmitry-kobak</id><content type="html" xml:base="https://ellis-life-heidelberg.eu//2021/06/16/ellis-life-data-science-seminar-dmitry-kobak.html">&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;I am going to present our recent work on manifold learning and low-dimensional visualizations of single-cell transcriptomic data. Single-cell transcriptomics yields ever growing datasets containing RNA expression levels for thousands of genes from up to millions of cells, often exhibiting rich hierarchical structure, both continuous and discrete. Common data analysis pipelines include dimensionality reduction for visualising the data in two dimensions, most frequently performed using methods like t-SNE, UMAP, and ForceAtlas2. These methods are all examples of &lt;em&gt;neighbour embeddings&lt;/em&gt;: their aim is to keep similar cells as neighbours in the embedding. The most established algorithm, t-SNE, excels at revealing local structure in high-dimensional data, but often struggles to represent the global structure accurately. I will discuss how much this applies to other neighbor embedding algorithms. I will show that changing the balance between the attractive and the repulsive forces yields a spectrum of embeddings, characterized by a simple trade-off: stronger attraction can better represent continuous manifold structures, while stronger repulsion can better represent discrete cluster structures. I will demonstrate that prominent neighbor embedding algorithms can all be placed onto this attraction-repulsion spectrum. I will elucidate other trade-offs, such as revealing coarser or finer cluster structure depending on the shape of the similarity kernel. Furthermore, I will demonstrate the influence of optimization parameters such as the learning rate and the initialization on the resulting embeddings. I will also discuss how to construct two-dimensional embeddings of other kinds of data, including library data and image data.&lt;/p&gt;

&lt;h3 id=&quot;biosketch&quot;&gt;Biosketch&lt;/h3&gt;

&lt;p&gt;Dmitry Kobak studied computer science and physics in St. Petersburg, Russia. He did PhD in computational neuroscience between Freiburg, Germany, and Imperial College London, UK, before moving to the Champalimaud Institute in Portugal for his postdoc. His interests gradually shifted towards data science and machine learning, and since 2017 he is working in the University of TÃ¼bingen, Germany, on data analysis of single-cell transcriptomic data. He is fascinated by manifold learning and dimensionality reduction, and is obsessed with figuring out how exactly various machine learning methods work.&lt;/p&gt;

&lt;p&gt;Website: &lt;a href=&quot;https://dkobak.github.io&quot;&gt;https://dkobak.github.io&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Abstract</summary></entry></feed>